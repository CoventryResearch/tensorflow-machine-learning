{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Recognition\n",
    "Our brains make vision seem easy. It doesn't take any effort for humans to tell apart a lion and a jaguar, read a sign, or recognize a human's face. But these are actually hard problems to solve with a computer: they only seem easy because our brains are incredibly good at understanding images.\n",
    "\n",
    "In the last few years the field of machine learning has made tremendous progress on addressing these difficult problems. In particular, we've found that a kind of model called a [deep convolutional neural network](https://colah.github.io/posts/2014-07-Conv-Nets-Modular/) can achieve reasonable performance on hard visual recognition tasks -- matching or exceeding human performance in some domains.\n",
    "\n",
    "Researchers have demonstrated steady progress in computer vision by validating their work against [ImageNet](http://www.image-net.org/) -- an academic benchmark for computer vision. Successive models continue to show improvements, each time achieving a new state-of-the-art result: [QuocNet](https://static.googleusercontent.com/media/research.google.com/en//archive/unsupervised_icml2012.pdf), [AlexNet](https://www.cs.toronto.edu/~fritz/absps/imagenet.pdf), [Inception (GoogLeNet)](https://arxiv.org/abs/1409.4842), [BN-Inception-v2](https://arxiv.org/abs/1502.03167). Researchers both internal and external to Google have published papers describing all these models but the results are still hard to reproduce. We're now taking the next step by releasing code for running image recognition on our latest model, [Inception-v3](https://arxiv.org/abs/1512.00567).\n",
    "\n",
    "Inception-v3 is trained for the [ImageNet](http://image-net.org/) Large Visual Recognition Challenge using the data from 2012. This is a standard task in computer vision, where models try to classify entire images into [1000 classes](http://image-net.org/challenges/LSVRC/2014/browse-synsets), like \"Zebra\", \"Dalmatian\", and \"Dishwasher\". For example, here are the results from AlexNet classifying some images:\n",
    "\n",
    "![AlexNet](AlexClassification.png)\n",
    "\n",
    "To compare models, we examine how often the model fails to predict the correct answer as one of their top 5 guesses -- termed \"top-5 error rate\". AlexNet achieved by setting a top-5 error rate of 15.3% on the 2012 validation data set; Inception (GoogLeNet) achieved 6.67%; BN-Inception-v2 achieved 4.9%; Inception-v3 reaches 3.46%.\n",
    "\n",
    "This tutorial will teach you how to use Inception-v3. You'll learn how to classify images into 1000 classes in Python or C++. We'll also discuss how to extract higher level features from this model which may be reused for other vision tasks.\n",
    "\n",
    "## Usage with Python API\n",
    "classify_image.py downloads the trained model from tensorflow.org when the program is run for the first time. You'll need about 200M of free space available on your hard disk. Let's run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Downloading inception-2015-12-05.tgz 100.0%\n",
      "Successfully downloaded inception-2015-12-05.tgz 88931400 bytes.\n",
      "giant panda, panda, panda bear, coon bear, Ailuropoda melanoleuca (score = 0.89107)\n",
      "indri, indris, Indri indri, Indri brevicaudatus (score = 0.00779)\n",
      "lesser panda, red panda, panda, bear cat, cat bear, Ailurus fulgens (score = 0.00296)\n",
      "custard apple (score = 0.00147)\n",
      "earthstar (score = 0.00117)\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/khanhnamle/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# %load classify_image.py\n",
    "\"\"\"Simple image classification with Inception.\n",
    "Run image classification with Inception trained on ImageNet 2012 Challenge data\n",
    "set.\n",
    "This program creates a graph from a saved GraphDef protocol buffer,\n",
    "and runs inference on an input JPEG image. It outputs human readable\n",
    "strings of the top 5 predictions along with their probabilities.\n",
    "Change the --image_file argument to any jpg image to compute a\n",
    "classification of that image.\n",
    "Please see the tutorial and website for a detailed description of how\n",
    "to use this script to perform image recognition.\n",
    "https://tensorflow.org/tutorials/image_recognition/\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import os.path\n",
    "import re\n",
    "import sys\n",
    "import tarfile\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "import tensorflow as tf\n",
    "\n",
    "FLAGS = None\n",
    "\n",
    "# pylint: disable=line-too-long\n",
    "DATA_URL = 'http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz'\n",
    "# pylint: enable=line-too-long\n",
    "\n",
    "\n",
    "class NodeLookup(object):\n",
    "  \"\"\"Converts integer node ID's to human readable labels.\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               label_lookup_path=None,\n",
    "               uid_lookup_path=None):\n",
    "    if not label_lookup_path:\n",
    "      label_lookup_path = os.path.join(\n",
    "          FLAGS.model_dir, 'imagenet_2012_challenge_label_map_proto.pbtxt')\n",
    "    if not uid_lookup_path:\n",
    "      uid_lookup_path = os.path.join(\n",
    "          FLAGS.model_dir, 'imagenet_synset_to_human_label_map.txt')\n",
    "    self.node_lookup = self.load(label_lookup_path, uid_lookup_path)\n",
    "\n",
    "  def load(self, label_lookup_path, uid_lookup_path):\n",
    "    \"\"\"Loads a human readable English name for each softmax node.\n",
    "    Args:\n",
    "      label_lookup_path: string UID to integer node ID.\n",
    "      uid_lookup_path: string UID to human-readable string.\n",
    "    Returns:\n",
    "      dict from integer node ID to human-readable string.\n",
    "    \"\"\"\n",
    "    if not tf.gfile.Exists(uid_lookup_path):\n",
    "      tf.logging.fatal('File does not exist %s', uid_lookup_path)\n",
    "    if not tf.gfile.Exists(label_lookup_path):\n",
    "      tf.logging.fatal('File does not exist %s', label_lookup_path)\n",
    "\n",
    "    # Loads mapping from string UID to human-readable string\n",
    "    proto_as_ascii_lines = tf.gfile.GFile(uid_lookup_path).readlines()\n",
    "    uid_to_human = {}\n",
    "    p = re.compile(r'[n\\d]*[ \\S,]*')\n",
    "    for line in proto_as_ascii_lines:\n",
    "      parsed_items = p.findall(line)\n",
    "      uid = parsed_items[0]\n",
    "      human_string = parsed_items[2]\n",
    "      uid_to_human[uid] = human_string\n",
    "\n",
    "    # Loads mapping from string UID to integer node ID.\n",
    "    node_id_to_uid = {}\n",
    "    proto_as_ascii = tf.gfile.GFile(label_lookup_path).readlines()\n",
    "    for line in proto_as_ascii:\n",
    "      if line.startswith('  target_class:'):\n",
    "        target_class = int(line.split(': ')[1])\n",
    "      if line.startswith('  target_class_string:'):\n",
    "        target_class_string = line.split(': ')[1]\n",
    "        node_id_to_uid[target_class] = target_class_string[1:-2]\n",
    "\n",
    "    # Loads the final mapping of integer node ID to human-readable string\n",
    "    node_id_to_name = {}\n",
    "    for key, val in node_id_to_uid.items():\n",
    "      if val not in uid_to_human:\n",
    "        tf.logging.fatal('Failed to locate: %s', val)\n",
    "      name = uid_to_human[val]\n",
    "      node_id_to_name[key] = name\n",
    "\n",
    "    return node_id_to_name\n",
    "\n",
    "  def id_to_string(self, node_id):\n",
    "    if node_id not in self.node_lookup:\n",
    "      return ''\n",
    "    return self.node_lookup[node_id]\n",
    "\n",
    "\n",
    "def create_graph():\n",
    "  \"\"\"Creates a graph from saved GraphDef file and returns a saver.\"\"\"\n",
    "  # Creates graph from saved graph_def.pb.\n",
    "  with tf.gfile.FastGFile(os.path.join(\n",
    "      FLAGS.model_dir, 'classify_image_graph_def.pb'), 'rb') as f:\n",
    "    graph_def = tf.GraphDef()\n",
    "    graph_def.ParseFromString(f.read())\n",
    "    _ = tf.import_graph_def(graph_def, name='')\n",
    "\n",
    "\n",
    "def run_inference_on_image(image):\n",
    "  \"\"\"Runs inference on an image.\n",
    "  Args:\n",
    "    image: Image file name.\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "  if not tf.gfile.Exists(image):\n",
    "    tf.logging.fatal('File does not exist %s', image)\n",
    "  image_data = tf.gfile.FastGFile(image, 'rb').read()\n",
    "\n",
    "  # Creates graph from saved GraphDef.\n",
    "  create_graph()\n",
    "\n",
    "  with tf.Session() as sess:\n",
    "    # Some useful tensors:\n",
    "    # 'softmax:0': A tensor containing the normalized prediction across\n",
    "    #   1000 labels.\n",
    "    # 'pool_3:0': A tensor containing the next-to-last layer containing 2048\n",
    "    #   float description of the image.\n",
    "    # 'DecodeJpeg/contents:0': A tensor containing a string providing JPEG\n",
    "    #   encoding of the image.\n",
    "    # Runs the softmax tensor by feeding the image_data as input to the graph.\n",
    "    softmax_tensor = sess.graph.get_tensor_by_name('softmax:0')\n",
    "    predictions = sess.run(softmax_tensor,\n",
    "                           {'DecodeJpeg/contents:0': image_data})\n",
    "    predictions = np.squeeze(predictions)\n",
    "\n",
    "    # Creates node ID --> English string lookup.\n",
    "    node_lookup = NodeLookup()\n",
    "\n",
    "    top_k = predictions.argsort()[-FLAGS.num_top_predictions:][::-1]\n",
    "    for node_id in top_k:\n",
    "      human_string = node_lookup.id_to_string(node_id)\n",
    "      score = predictions[node_id]\n",
    "      print('%s (score = %.5f)' % (human_string, score))\n",
    "\n",
    "\n",
    "def maybe_download_and_extract():\n",
    "  \"\"\"Download and extract model tar file.\"\"\"\n",
    "  dest_directory = FLAGS.model_dir\n",
    "  if not os.path.exists(dest_directory):\n",
    "    os.makedirs(dest_directory)\n",
    "  filename = DATA_URL.split('/')[-1]\n",
    "  filepath = os.path.join(dest_directory, filename)\n",
    "  if not os.path.exists(filepath):\n",
    "    def _progress(count, block_size, total_size):\n",
    "      sys.stdout.write('\\r>> Downloading %s %.1f%%' % (\n",
    "          filename, float(count * block_size) / float(total_size) * 100.0))\n",
    "      sys.stdout.flush()\n",
    "    filepath, _ = urllib.request.urlretrieve(DATA_URL, filepath, _progress)\n",
    "    print()\n",
    "    statinfo = os.stat(filepath)\n",
    "    print('Successfully downloaded', filename, statinfo.st_size, 'bytes.')\n",
    "  tarfile.open(filepath, 'r:gz').extractall(dest_directory)\n",
    "\n",
    "\n",
    "def main(_):\n",
    "  maybe_download_and_extract()\n",
    "  image = (FLAGS.image_file if FLAGS.image_file else\n",
    "           os.path.join(FLAGS.model_dir, 'cropped_panda.jpg'))\n",
    "  run_inference_on_image(image)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  parser = argparse.ArgumentParser()\n",
    "  # classify_image_graph_def.pb:\n",
    "  #   Binary representation of the GraphDef protocol buffer.\n",
    "  # imagenet_synset_to_human_label_map.txt:\n",
    "  #   Map from synset ID to a human readable string.\n",
    "  # imagenet_2012_challenge_label_map_proto.pbtxt:\n",
    "  #   Text representation of a protocol buffer mapping a label to synset ID.\n",
    "  parser.add_argument(\n",
    "      '--model_dir',\n",
    "      type=str,\n",
    "      default='/tmp/imagenet',\n",
    "      help=\"\"\"\\\n",
    "      Path to classify_image_graph_def.pb,\n",
    "      imagenet_synset_to_human_label_map.txt, and\n",
    "      imagenet_2012_challenge_label_map_proto.pbtxt.\\\n",
    "      \"\"\"\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--image_file',\n",
    "      type=str,\n",
    "      default='',\n",
    "      help='Absolute path to image file.'\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--num_top_predictions',\n",
    "      type=int,\n",
    "      default=5,\n",
    "      help='Display this many predictions.'\n",
    "  )\n",
    "  FLAGS, unparsed = parser.parse_known_args()\n",
    "  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The above command will classify a supplied image of a panda bear.\n",
    "\n",
    "![Panda](cropped_panda.jpg)\n",
    "\n",
    "## Usage with the C++ API\n",
    "You can run the same Inception-v3 model in C++ for use in production environments. You can download the archive containing the GraphDef that defines the model like this (running from the root directory of the TensorFlow repository):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "curl -L \"https://storage.googleapis.com/download.tensorflow.org/models/inception_v3_2016_08_28_frozen.pb.tar.gz\" |\n",
    "  tar -C tensorflow/examples/label_image/data -xz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to compile the C++ binary that includes the code to load and run the graph. If you've followed [the instructions to download the source installation of TensorFlow](https://www.tensorflow.org/install/install_sources) for your platform, you should be able to build the example by running this command from your shell terminal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bazel build tensorflow/examples/label_image/..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That should create a binary executable that you can then run like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bazel-bin/tensorflow/examples/label_image/label_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This uses the default example image that ships with the framework, and should output something similar to this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "I tensorflow/examples/label_image/main.cc:206] military uniform (653): 0.834306\n",
    "I tensorflow/examples/label_image/main.cc:206] mortarboard (668): 0.0218692\n",
    "I tensorflow/examples/label_image/main.cc:206] academic gown (401): 0.0103579\n",
    "I tensorflow/examples/label_image/main.cc:206] pickelhaube (716): 0.00800814\n",
    "I tensorflow/examples/label_image/main.cc:206] bulletproof vest (466): 0.00535088"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we're using the default image of [Admiral Grace Hopper](https://en.wikipedia.org/wiki/Grace_Hopper), and you can see the network correctly identifies she's wearing a military uniform, with a high score of 0.8.\n",
    "\n",
    "![GraceHopper](grace_hopper.jpg)\n",
    "\n",
    "Next, try it out on your own images by supplying the --image= argument, e.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bazel-bin/tensorflow/examples/label_image/label_image --image=my_image.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look inside the [tensorflow/examples/label_image/main.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/label_image/main.cc) file, you can find out how it works. We hope this code will help you integrate TensorFlow into your own applications, so we will walk step by step through the main functions:\n",
    "\n",
    "The command line flags control where the files are loaded from, and properties of the input images. The model expects to get square 299x299 RGB images, so those are the input_width and input_height flags. We also need to scale the pixel values from integers that are between 0 and 255 to the floating point values that the graph operates on. We control the scaling with the input_mean and input_std flags: we first subtract input_mean from each pixel value, then divide it by input_std.\n",
    "\n",
    "These values probably look somewhat magical, but they are just defined by the original model author based on what he/she wanted to use as input images for training. If you have a graph that you've trained yourself, you'll just need to adjust the values to match whatever you used during your training process.\n",
    "\n",
    "You can see how they're applied to an image in the ReadTensorFromImageFile() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Given an image file name, read in the data, try to decode it as an image,\n",
    "// resize it to the requested size, and then scale the values as desired.\n",
    "Status ReadTensorFromImageFile(string file_name, const int input_height,\n",
    "                               const int input_width, const float input_mean,\n",
    "                               const float input_std,\n",
    "                               std::vector<Tensor>* out_tensors) {\n",
    "  tensorflow::GraphDefBuilder b;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by creating a GraphDefBuilder, which is an object we can use to specify a model to run or load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "string input_name = \"file_reader\";\n",
    "string output_name = \"normalized\";\n",
    "tensorflow::Node* file_reader =\n",
    "tensorflow::ops::ReadFile(tensorflow::ops::Const(file_name, b.opts()),\n",
    "                    b.opts().WithName(input_name));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then start creating nodes for the small model we want to run to load, resize, and scale the pixel values to get the result the main model expects as its input. The first node we create is just a Const op that holds a tensor with the file name of the image we want to load. That's then passed as the first input to the ReadFile op. You might notice we're passing b.opts() as the last argument to all the op creation functions. The argument ensures that the node is added to the model definition held in the GraphDefBuilder. We also name the ReadFile operator by making the WithName() call to b.opts(). This gives a name to the node, which isn't strictly necessary since an automatic name will be assigned if you don't do this, but it does make debugging a bit easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "  // Now try to figure out what kind of file it is and decode it.\n",
    "  const int wanted_channels = 3;\n",
    "  tensorflow::Node* image_reader;\n",
    "  if (tensorflow::StringPiece(file_name).ends_with(\".png\")) {\n",
    "    image_reader = tensorflow::ops::DecodePng(\n",
    "        file_reader,\n",
    "        b.opts().WithAttr(\"channels\", wanted_channels).WithName(\"png_reader\"));\n",
    "  } else {\n",
    "    // Assume if it's not a PNG then it must be a JPEG.\n",
    "    image_reader = tensorflow::ops::DecodeJpeg(\n",
    "        file_reader,\n",
    "        b.opts().WithAttr(\"channels\", wanted_channels).WithName(\"jpeg_reader\"));\n",
    "  }\n",
    "  // Now cast the image data to float so we can do normal math on it.\n",
    "  tensorflow::Node* float_caster = tensorflow::ops::Cast(\n",
    "      image_reader, tensorflow::DT_FLOAT, b.opts().WithName(\"float_caster\"));\n",
    "  // The convention for image ops in TensorFlow is that all images are expected\n",
    "  // to be in batches, so that they're four-dimensional arrays with indices of\n",
    "  // [batch, height, width, channel]. Because we only have a single image, we\n",
    "  // have to add a batch dimension of 1 to the start with ExpandDims().\n",
    "  tensorflow::Node* dims_expander = tensorflow::ops::ExpandDims(\n",
    "      float_caster, tensorflow::ops::Const(0, b.opts()), b.opts());\n",
    "  // Bilinearly resize the image to fit the required dimensions.\n",
    "  tensorflow::Node* resized = tensorflow::ops::ResizeBilinear(\n",
    "      dims_expander, tensorflow::ops::Const({input_height, input_width},\n",
    "                                            b.opts().WithName(\"size\")),\n",
    "      b.opts());\n",
    "  // Subtract the mean and divide by the scale.\n",
    "  tensorflow::ops::Div(\n",
    "      tensorflow::ops::Sub(\n",
    "          resized, tensorflow::ops::Const({input_mean}, b.opts()), b.opts()),\n",
    "      tensorflow::ops::Const({input_std}, b.opts()),\n",
    "      b.opts().WithName(output_name));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then keep adding more nodes, to decode the file data as an image, to cast the integers into floating point values, to resize it, and then finally to run the subtraction and division operations on the pixel values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "  // This runs the GraphDef network definition that we've just constructed, and\n",
    "  // returns the results in the output tensor.\n",
    "  tensorflow::GraphDef graph;\n",
    "  TF_RETURN_IF_ERROR(b.ToGraphDef(&graph));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of this we have a model definition stored in the b variable, which we turn into a full graph definition with the ToGraphDef() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "  std::unique_ptr<tensorflow::Session> session(\n",
    "      tensorflow::NewSession(tensorflow::SessionOptions()));\n",
    "  TF_RETURN_IF_ERROR(session->Create(graph));\n",
    "  TF_RETURN_IF_ERROR(session->Run({}, {output_name}, {}, out_tensors));\n",
    "  return Status::OK();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create a tf.Session object, which is the interface to actually running the graph, and run it, specifying which node we want to get the output from, and where to put the output data.\n",
    "\n",
    "This gives us a vector of Tensor objects, which in this case we know will only be a single object long. You can think of a Tensor as a multi-dimensional array in this context, and it holds a 299 pixel high, 299 pixel wide, 3 channel image as float values. If you have your own image-processing framework in your product already, you should be able to use that instead, as long as you apply the same transformations before you feed images into the main graph.\n",
    "\n",
    "This is a simple example of creating a small TensorFlow graph dynamically in C++, but for the pre-trained Inception model we want to load a much larger definition from a file. You can see how we do that in the LoadGraph() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Reads a model graph definition from disk, and creates a session object you\n",
    "// can use to run it.\n",
    "Status LoadGraph(string graph_file_name,\n",
    "                 std::unique_ptr<tensorflow::Session>* session) {\n",
    "  tensorflow::GraphDef graph_def;\n",
    "  Status load_graph_status =\n",
    "      ReadBinaryProto(tensorflow::Env::Default(), graph_file_name, &graph_def);\n",
    "  if (!load_graph_status.ok()) {\n",
    "    return tensorflow::errors::NotFound(\"Failed to load compute graph at '\",\n",
    "                                        graph_file_name, \"'\");\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you've looked through the image loading code, a lot of the terms should seem familiar. Rather than using a GraphDefBuilder to produce a GraphDef object, we load a protobuf file that directly contains the GraphDef."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "  session->reset(tensorflow::NewSession(tensorflow::SessionOptions()));\n",
    "  Status session_create_status = (*session)->Create(graph_def);\n",
    "  if (!session_create_status.ok()) {\n",
    "    return session_create_status;\n",
    "  }\n",
    "  return Status::OK();\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create a Session object from that GraphDef and pass it back to the caller so that they can run it at a later time.\n",
    "\n",
    "The GetTopLabels() function is a lot like the image loading, except that in this case we want to take the results of running the main graph, and turn it into a sorted list of the highest-scoring labels. Just like the image loader, it creates a GraphDefBuilder, adds a couple of nodes to it, and then runs the short graph to get a pair of output tensors. In this case they represent the sorted scores and index positions of the highest results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Analyzes the output of the Inception graph to retrieve the highest scores and\n",
    "// their positions in the tensor, which correspond to categories.\n",
    "Status GetTopLabels(const std::vector<Tensor>& outputs, int how_many_labels,\n",
    "                    Tensor* indices, Tensor* scores) {\n",
    "  tensorflow::GraphDefBuilder b;\n",
    "  string output_name = \"top_k\";\n",
    "  tensorflow::ops::TopK(tensorflow::ops::Const(outputs[0], b.opts()),\n",
    "                        how_many_labels, b.opts().WithName(output_name));\n",
    "  // This runs the GraphDef network definition that we've just constructed, and\n",
    "  // returns the results in the output tensors.\n",
    "  tensorflow::GraphDef graph;\n",
    "  TF_RETURN_IF_ERROR(b.ToGraphDef(&graph));\n",
    "  std::unique_ptr<tensorflow::Session> session(\n",
    "      tensorflow::NewSession(tensorflow::SessionOptions()));\n",
    "  TF_RETURN_IF_ERROR(session->Create(graph));\n",
    "  // The TopK node returns two outputs, the scores and their original indices,\n",
    "  // so we have to append :0 and :1 to specify them both.\n",
    "  std::vector<Tensor> out_tensors;\n",
    "  TF_RETURN_IF_ERROR(session->Run({}, {output_name + \":0\", output_name + \":1\"},\n",
    "                                  {}, &out_tensors));\n",
    "  *scores = out_tensors[0];\n",
    "  *indices = out_tensors[1];\n",
    "  return Status::OK();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PrintTopLabels() function takes those sorted results, and prints them out in a friendly way. The CheckTopLabel() function is very similar, but just makes sure that the top label is the one we expect, for debugging purposes.\n",
    "\n",
    "At the end, main() ties together all of these calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "int main(int argc, char* argv[]) {\n",
    "  // We need to call this to set up global state for TensorFlow.\n",
    "  tensorflow::port::InitMain(argv[0], &argc, &argv);\n",
    "  Status s = tensorflow::ParseCommandLineFlags(&argc, argv);\n",
    "  if (!s.ok()) {\n",
    "    LOG(ERROR) << \"Error parsing command line flags: \" << s.ToString();\n",
    "    return -1;\n",
    "  }\n",
    "\n",
    "  // First we load and initialize the model.\n",
    "  std::unique_ptr<tensorflow::Session> session;\n",
    "  string graph_path = tensorflow::io::JoinPath(FLAGS_root_dir, FLAGS_graph);\n",
    "  Status load_graph_status = LoadGraph(graph_path, &session);\n",
    "  if (!load_graph_status.ok()) {\n",
    "    LOG(ERROR) << load_graph_status;\n",
    "    return -1;\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the main graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "  // Get the image from disk as a float array of numbers, resized and normalized\n",
    "  // to the specifications the main graph expects.\n",
    "  std::vector<Tensor> resized_tensors;\n",
    "  string image_path = tensorflow::io::JoinPath(FLAGS_root_dir, FLAGS_image);\n",
    "  Status read_tensor_status = ReadTensorFromImageFile(\n",
    "      image_path, FLAGS_input_height, FLAGS_input_width, FLAGS_input_mean,\n",
    "      FLAGS_input_std, &resized_tensors);\n",
    "  if (!read_tensor_status.ok()) {\n",
    "    LOG(ERROR) << read_tensor_status;\n",
    "    return -1;\n",
    "  }\n",
    "  const Tensor& resized_tensor = resized_tensors[0];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load, resize, and process the input image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "  // Actually run the image through the model.\n",
    "  std::vector<Tensor> outputs;\n",
    "  Status run_status = session->Run({ {FLAGS_input_layer, resized_tensor}},\n",
    "                                   {FLAGS_output_layer}, {}, &outputs);\n",
    "  if (!run_status.ok()) {\n",
    "    LOG(ERROR) << \"Running model failed: \" << run_status;\n",
    "    return -1;\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we run the loaded graph with the image as an input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "  // This is for automated testing to make sure we get the expected result with\n",
    "  // the default settings. We know that label 866 (military uniform) should be\n",
    "  // the top label for the Admiral Hopper image.\n",
    "  if (FLAGS_self_test) {\n",
    "    bool expected_matches;\n",
    "    Status check_status = CheckTopLabel(outputs, 866, &expected_matches);\n",
    "    if (!check_status.ok()) {\n",
    "      LOG(ERROR) << \"Running check failed: \" << check_status;\n",
    "      return -1;\n",
    "    }\n",
    "    if (!expected_matches) {\n",
    "      LOG(ERROR) << \"Self-test failed!\";\n",
    "      return -1;\n",
    "    }\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For testing purposes we can check to make sure we get the output we expect here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "  // Do something interesting with the results we've generated.\n",
    "  Status print_status = PrintTopLabels(outputs, FLAGS_labels);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we print the labels we found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "  if (!print_status.ok()) {\n",
    "    LOG(ERROR) << \"Running print failed: \" << print_status;\n",
    "    return -1;\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error handling here is using TensorFlow's Status object, which is very convenient because it lets you know whether any error has occurred with the ok() checker, and then can be printed out to give a readable error message.\n",
    "\n",
    "In this case we are demonstrating object recognition, but you should be able to use very similar code on other models you've found or trained yourself, across all sorts of domains. We hope this small example gives you some ideas on how to use TensorFlow within your own products."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
