{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving Linear Models Using Explicit Kernel Methods\n",
    "In this tutorial, we demonstrate how combining (explicit) kernel methods with linear models can drastically increase the latters' quality of predictions without significantly increasing training and inference times. Unlike dual kernel methods, explicit (primal) kernel methods scale well with the size of the training dataset both in terms of training/inference times and in terms of memory requirements.\n",
    "\n",
    "Intended audience: Even though we provide a high-level overview of concepts related to explicit kernel methods, this tutorial primarily targets readers who already have at least basic knowledge of kernel methods and Support Vector Machines (SVMs). If you are new to kernel methods, refer to either of the following sources for an introduction:\n",
    "\n",
    "* If you have a strong mathematical background: [Kernel Methods in Machine Learning](https://arxiv.org/pdf/math/0701907.pdf)\n",
    "* [Kernel method wikipedia page](https://en.wikipedia.org/wiki/Kernel_method)\n",
    "\n",
    "Currently, TensorFlow supports explicit kernel mappings for dense features only; TensorFlow will provide support for sparse features at a later release.\n",
    "\n",
    "This tutorial uses [tf.contrib.learn](https://github.com/tensorflow/tensorflow/tree/r1.5/tensorflow/contrib/learn/python/learn) (TensorFlow's high-level Machine Learning API) Estimators for our ML models. If you are not familiar with this API, [tf.estimator](https://www.tensorflow.org/get_started/premade_estimators) Quickstart is a good place to start. We will use the MNIST dataset. The tutorial consists of the following steps:\n",
    "\n",
    "* Load and prepare MNIST data for classification.\n",
    "* Construct a simple linear model, train it, and evaluate it on the eval data.\n",
    "* Replace the linear model with a kernelized linear model, re-train, and re-evaluate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and prepare MNIST data for classification\n",
    "Run the following utility command to load the MNIST dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = tf.contrib.learn.datasets.mnist.load_mnist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preceding method loads the entire MNIST dataset (containing 70K samples) and splits it into train, validation, and test data with 55K, 5K, and 10K samples respectively. Each split contains one numpy array for images (with shape [sample_size, 784]) and one for labels (with shape [sample_size, 1]). In this tutorial, we only use the train and validation splits to train and evaluate our models respectively.\n",
    "\n",
    "In order to feed data to a tf.contrib.learn Estimator, it is helpful to convert it to Tensors. For this, we will use an input function which adds Ops to the TensorFlow graph that, when executed, create mini-batches of Tensors to be used downstream. For more background on input functions, check [this section on input functions](https://www.tensorflow.org/get_started/premade_estimators#input_fn). In this example, we will use the tf.train.shuffle_batch Op which, besides converting numpy arrays to Tensors, allows us to specify the batch_size and whether to randomize the input every time the input_fn Ops are executed (randomization typically expedites convergence during training). The full code for loading and preparing the data is shown in the snippet below. In this example, we use mini-batches of size 256 for training and the entire sample (5K entries) for evaluation. Feel free to experiment with different batch sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def get_input_fn(dataset_split, batch_size, capacity=10000, min_after_dequeue=3000):\n",
    "\n",
    "  def _input_fn():\n",
    "    images_batch, labels_batch = tf.train.shuffle_batch(\n",
    "        tensors=[dataset_split.images, dataset_split.labels.astype(np.int32)],\n",
    "        batch_size=batch_size,\n",
    "        capacity=capacity,\n",
    "        min_after_dequeue=min_after_dequeue,\n",
    "        enqueue_many=True,\n",
    "        num_threads=4)\n",
    "    features_map = {'images': images_batch}\n",
    "    return features_map, labels_batch\n",
    "\n",
    "  return _input_fn\n",
    "\n",
    "data = tf.contrib.learn.datasets.mnist.load_mnist()\n",
    "\n",
    "train_input_fn = get_input_fn(data.train, batch_size=256)\n",
    "eval_input_fn = get_input_fn(data.validation, batch_size=5000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a simple linear model\n",
    "We can now train a linear model over the MNIST dataset. We will use the [tf.contrib.learn.LinearClassifier](https://www.tensorflow.org/api_docs/python/tf/contrib/learn/LinearClassifier) estimator with 10 classes representing the 10 digits. The input features form a 784-dimensional dense vector which can be specified as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_column = tf.contrib.layers.real_valued_column('images', dimension=784)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full code for constructing, training and evaluating a LinearClassifier estimator is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Specify the feature(s) to be used by the estimator.\n",
    "image_column = tf.contrib.layers.real_valued_column('images', dimension=784)\n",
    "estimator = tf.contrib.learn.LinearClassifier(feature_columns=[image_column], n_classes=10)\n",
    "\n",
    "# Train.\n",
    "start = time.time()\n",
    "estimator.fit(input_fn=train_input_fn, steps=2000)\n",
    "end = time.time()\n",
    "print('Elapsed time: {} seconds'.format(end - start))\n",
    "\n",
    "# Evaluate and report metrics.\n",
    "eval_metrics = estimator.evaluate(input_fn=eval_input_fn, steps=1)\n",
    "print(eval_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following table summarizes the results on the eval data (metric\tand value):\n",
    "* loss: 0.25 to 0.30\n",
    "* accuracy: 92.5%\n",
    "* training time: ~25 seconds on my machine\n",
    "\n",
    "In addition to experimenting with the (training) batch size and the number of training steps, there are a couple other parameters that can be tuned as well. For instance, you can change the optimization method used to minimize the loss by explicitly selecting another optimizer from the collection of [available optimizers](https://github.com/tensorflow/tensorflow/tree/r1.5/tensorflow/python/training). As an example, the following code constructs a LinearClassifier estimator that uses the Follow-The-Regularized-Leader (FTRL) optimization strategy with a specific learning rate and L2-regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.FtrlOptimizer(learning_rate=5.0, l2_regularization_strength=1.0)\n",
    "estimator = tf.contrib.learn.LinearClassifier(\n",
    "    feature_columns=[image_column], n_classes=10, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regardless of the values of the parameters, the maximum accuracy a linear model can achieve on this dataset caps at around 93%.\n",
    "\n",
    "## Using explicit kernel mappings with the linear model.\n",
    "The relatively high error (~7%) of the linear model over MNIST indicates that the input data is not linearly separable. We will use explicit kernel mappings to reduce the classification error.\n",
    "\n",
    "Intuition: The high-level idea is to use a non-linear map to transform the input space to another feature space (of possibly higher dimension) where the (transformed) features are (almost) linearly separable and then apply a linear model on the mapped features. This is shown in the following figure:\n",
    "\n",
    "![kernel-mapping](kernel_mapping.png)\n",
    "\n",
    "### Technical details\n",
    "In this example we will use Random Fourier Features, introduced in the [\"Random Features for Large-Scale Kernel Machines\"](https://people.eecs.berkeley.edu/~brecht/papers/07.rah.rec.nips.pdf) paper by Rahimi and Recht, to map the input data. Random Fourier Features map a vector $\\mathbf{x} \\in \\mathbb{R}^d$ to $\\mathbf{x'} \\in \\mathbb{R}^D$ via the following mapping:\n",
    "\n",
    "$$RFFM(\\cdot): \\mathbb{R}^d \\to \\mathbb{R}^D, \\quad\n",
    "RFFM(\\mathbf{x}) =  \\cos(\\mathbf{\\Omega} \\cdot \\mathbf{x}+ \\mathbf{b})$$\n",
    "\n",
    "where $\\mathbf{\\Omega} \\in \\mathbb{R}^{D \\times d}$, $\\mathbf{x} \\in \\mathbb{R}^d,$ $\\mathbf{b} \\in \\mathbb{R}^D$ and the cosine is applied element-wise.\n",
    "\n",
    "In this example, the entries of $\\mathbf{\\Omega}$ and $\\mathbf{b}$ are sampled from distributions such that the mapping satisfies the following property:\n",
    "\n",
    "$$RFFM(\\mathbf{x})^T \\cdot RFFM(\\mathbf{y}) \\approx\n",
    "e^{-\\frac{\\|\\mathbf{x} - \\mathbf{y}\\|^2}{2 \\sigma^2}}$$\n",
    "\n",
    "The right-hand-side quantity of the expression above is known as the RBF (or Gaussian) kernel function. This function is one of the most-widely used kernel functions in Machine Learning and implicitly measures similarity in a different, much higher dimensional space than the original one. See [Radial basis function kernel](https://en.wikipedia.org/wiki/Radial_basis_function_kernel) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel classifier\n",
    "[tf.contrib.kernel_methods.KernelLinearClassifier](https://www.tensorflow.org/api_docs/python/tf/contrib/kernel_methods/KernelLinearClassifier) is a pre-packaged tf.contrib.learn estimator that combines the power of explicit kernel mappings with linear models. Its constructor is almost identical to that of the LinearClassifier estimator with the additional option to specify a list of explicit kernel mappings to be applied to each feature the classifier uses. The following code snippet demonstrates how to replace LinearClassifier with KernelLinearClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Specify the feature(s) to be used by the estimator. This is identical to the\n",
    "# code used for the LinearClassifier.\n",
    "image_column = tf.contrib.layers.real_valued_column('images', dimension=784)\n",
    "optimizer = tf.train.FtrlOptimizer(\n",
    "   learning_rate=50.0, l2_regularization_strength=0.001)\n",
    "\n",
    "kernel_mapper = tf.contrib.kernel_methods.RandomFourierFeatureMapper(\n",
    "  input_dim=784, output_dim=2000, stddev=5.0, name='rffm')\n",
    "kernel_mappers = {image_column: [kernel_mapper]}\n",
    "estimator = tf.contrib.kernel_methods.KernelLinearClassifier(\n",
    "   n_classes=10, optimizer=optimizer, kernel_mappers=kernel_mappers)\n",
    "\n",
    "# Train.\n",
    "start = time.time()\n",
    "estimator.fit(input_fn=train_input_fn, steps=2000)\n",
    "end = time.time()\n",
    "print('Elapsed time: {} seconds'.format(end - start))\n",
    "\n",
    "# Evaluate and report metrics.\n",
    "eval_metrics = estimator.evaluate(input_fn=eval_input_fn, steps=1)\n",
    "print(eval_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only additional parameter passed to KernelLinearClassifier is a dictionary from feature_columns to a list of kernel mappings to be applied to the corresponding feature column. The following lines instruct the classifier to first map the initial 784-dimensional images to 2000-dimensional vectors using random Fourier features and then learn a linear model on the transformed vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kernel_mapper = tf.contrib.kernel_methods.RandomFourierFeatureMapper(\n",
    "  input_dim=784, output_dim=2000, stddev=5.0, name='rffm')\n",
    "kernel_mappers = {image_column: [kernel_mapper]}\n",
    "estimator = tf.contrib.kernel_methods.KernelLinearClassifier(\n",
    "   n_classes=10, optimizer=optimizer, kernel_mappers=kernel_mappers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the stddev parameter. This is the standard deviation () of the approximated RBF kernel and controls the similarity measure used in classification. stddev is typically determined via hyperparameter tuning.\n",
    "\n",
    "The results of running the preceding code are summarized in the following table. We can further increase the accuracy by increasing the output dimension of the mapping and tuning the standard deviation.\n",
    "\n",
    "Metric and Value:\n",
    "* loss: 0.10\n",
    "* accuracy: 97%\n",
    "* training time: ~35 seconds on my machine\n",
    "\n",
    "### stddev\n",
    "The classification quality is very sensitive to the value of stddev. The following table shows the accuracy of the classifier on the eval data for different values of stddev. The optimal value is stddev=5.0. Notice how too small or too high stddev values can dramatically decrease the accuracy of the classification.\n",
    "\n",
    "stddev and eval accuracy\n",
    "* 1.0:\t0.1362\n",
    "* 2.0:\t0.4764\n",
    "* 4.0:\t0.9654\n",
    "* 5.0:\t0.9766\n",
    "* 8.0:\t0.9714\n",
    "* 16.0:\t0.8878\n",
    "\n",
    "### Output dimension\n",
    "Intuitively, the larger the output dimension of the mapping, the closer the inner product of two mapped vectors approximates the kernel, which typically translates to better classification accuracy. Another way to think about this is that the output dimension equals the number of weights of the linear model; the larger this dimension, the larger the \"degrees of freedom\" of the model. However, after a certain threshold, higher output dimensions increase the accuracy by very little, while making training take more time. This is shown in the following two Figures which depict the eval accuracy as a function of the output dimension and the training time, respectively.\n",
    "\n",
    "![acc-outdim](acc_vs_outdim.png)\n",
    "\n",
    "![acc-trn-time](acc-vs-trn_time.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "Explicit kernel mappings combine the predictive power of nonlinear models with the scalability of linear models. Unlike traditional dual kernel methods, explicit kernel methods can scale to millions or hundreds of millions of samples. When using explicit kernel mappings, consider the following tips:\n",
    "\n",
    "* Random Fourier Features can be particularly effective for datasets with dense features.\n",
    "* The parameters of the kernel mapping are often data-dependent. Model quality can be very sensitive to these parameters. Use hyperparameter tuning to find the optimal values.\n",
    "* If you have multiple numerical features, concatenate them into a single multi-dimensional feature and apply the kernel mapping to the concatenated vector."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
